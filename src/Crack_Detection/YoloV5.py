# -*- coding: utf-8 -*-
"""SSUP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nblz72tSXmAKOp1UTn1Axy1M_TgFA2sb

Pipeline Crack Detection and Classification
"""

# Commented out IPython magic to ensure Python compatibility.
#Installing libraries and cloning Github

#git clone https://github.com/ultralytics/yolov5.git
# %cd yolov5
#pip install -q torch torchvision ultralytics
import pandas as pd
import matplotlib.pyplot as plt
#pip install -qr requirements.txt
import matplotlib.image as mpimg
import zipfile
import os
from PIL import Image
import numpy as np

from google.colab import files

# Upload the pipeline.zip
print("Please upload your pipeline.zip containing Train, Test, and Val folders.")
uploaded = files.upload()

# Extract the zip file
dataset_zip = list(uploaded.keys())[0]
dataset_dir = "/content/pipeline"
with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
    zip_ref.extractall(dataset_dir)

print(f"Dataset extracted to: {dataset_dir}")

# Verify the extracted dataset structure
for root, dirs, files in os.walk(dataset_dir):
    print(root, dirs, files)

# Moving Labels to respective Images folder
!mv /content/pipeline/pipeline/Train/Labels/*.txt /content/pipeline/pipeline/Train/Images/
!mv /content/pipeline/pipeline/Val/Labels/*.txt /content/pipeline/pipeline/Val/Images/
!mv /content/pipeline/pipeline/Test/Labels/*.txt /content/pipeline/pipeline/Test/Images/

data_yaml = f"""
train: {os.path.join(dataset_dir, 'Train/Images')}  # Path to training images
val: {os.path.join(dataset_dir, 'Val/Images')}  # Path to validation images
nc: 3  # No. of classes
names: ['Minor', 'Moderate', 'Severe']  # Class names in your dataset

#To avoid Overfitting and Generalisation
augmentation:
  flipud: 0.5  # Flip the image vertically with a 50% probability
  fliplr: 0.5  # Flip the image horizontally with a 50% probability
  hsv_h: 0.02  # Randomly adjust the hue by ±0.02 (hue shifts color tones)
  rotate: 15  # Randomly rotate the image by ±15 degrees (for rotation invariance)
  """

!python train.py \
  --img 640 \  # Images will be resized to 640x640
  --batch 16 \  # Each batch will contain 16 images.
  --epochs 50 \  # Number of epochs
  --data /content/pipeline/pipeline/yolo_data.yaml \  # Path to the YAML file containing the dataset configuration.
  --weights yolov5s.pt \  # Loads pre-trained YOLOv5 small model weights.
  --project /content/yolov5_training \  # Directory where the training output will be saved
  --name crack_detection_model \  # It will create a folder with this name inside the project directory to store results.
  --cache \  # Caches the dataset images
  --patience 10 \  # Number of epochs with no improvement in the validation loss

#Checking the training results folder

!ls /content/yolov5_training/crack_detection_model9

# Load the training results CSV file
results_path = '/content/yolov5_training/crack_detection_model9/results.csv'
results = pd.read_csv(results_path)

# Clean column names by stripping extra spaces
results.columns = results.columns.str.strip()

print(results.head())

# Commented out IPython magic to ensure Python compatibility.
#Displaying Results

results_dir = "/content/yolov5_training/crack_detection_model9"
# %matplotlib inline

# Plot 1: Training and validation box loss
plt.figure(figsize=(10, 6))
plt.plot(results['epoch'], results['train/box_loss'], label='Train Box Loss')
plt.plot(results['epoch'], results['val/box_loss'], label='Val Box Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Box Loss')
plt.legend()
plt.show()

# Plot 2: Training and validation objectness loss
plt.figure(figsize=(10, 6))
plt.plot(results['epoch'], results['train/obj_loss'], label='Train Obj Loss')
plt.plot(results['epoch'], results['val/obj_loss'], label='Val Obj Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Objectness Loss')
plt.legend()
plt.show()

# Plot 3: Training and validation classification loss
plt.figure(figsize=(10, 6))
plt.plot(results['epoch'], results['train/cls_loss'], label='Train Cls Loss')
plt.plot(results['epoch'], results['val/cls_loss'], label='Val Cls Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Classification Loss')
plt.legend()
plt.show()

# Plot 4: Precision over epochs
plt.figure(figsize=(10, 6))
plt.plot(results['epoch'], results['metrics/precision'], label='Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.title('Precision')
plt.legend()
plt.show()

# Plot 5: Recall over epochs
plt.figure(figsize=(10, 6))
plt.plot(results['epoch'], results['metrics/recall'], label='Recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.title('Recall')
plt.legend()
plt.show()

# Plot 6: mAP values over epochs
plt.figure(figsize=(10, 6))
plt.plot(results['epoch'], results['metrics/mAP_0.5'], label='mAP@0.5')
plt.plot(results['epoch'], results['metrics/mAP_0.5:0.95'], label='mAP@0.5:0.95')
plt.xlabel('Epoch')
plt.ylabel('mAP')
plt.title('Mean Average Precision (mAP)')
plt.legend()
plt.show()

# Plot 7: Display Confusion Matrix
confusion_matrix_path = os.path.join(results_dir, "confusion_matrix.png")
if os.path.exists(confusion_matrix_path):
    img = mpimg.imread(confusion_matrix_path)
    plt.imshow(img)
    plt.title('Confusion Matrix')
    plt.axis('off')
    plt.show()

# Plot 8: Display Precision-Recall Curve
pr_curve_path = os.path.join(results_dir, "PR_curve.png")
if os.path.exists(pr_curve_path):
    img = mpimg.imread(pr_curve_path)
    plt.imshow(img)
    plt.title('Precision-Recall Curve')
    plt.axis('off')
    plt.show()

# Plot 9: Display example images with predictions
image_files = ["val_batch0_pred.jpg", "val_batch1_pred.jpg", "val_batch2_pred.jpg"]  # Add more if needed
for image_file in image_files:
    image_path = os.path.join(results_dir, image_file)
    if os.path.exists(image_path):
        img = mpimg.imread(image_path)
        plt.imshow(img)
        plt.title(f'Predicted Image: {image_file}')
        plt.axis('off')
        plt.show()

# Checking for overfitting
final_epoch = results['epoch'].iloc[-1]
train_box_loss = results['train/box_loss'].iloc[-1]
val_box_loss = results['val/box_loss'].iloc[-1]
if val_box_loss > train_box_loss and val_box_loss - train_box_loss > 0.05:
    print("Warning: Overfitting detected. Validation loss is higher than training loss.")
else:
    print("No significant overfitting detected.")

/content/yolov5/detect.py --weights /content/yolov5_training/crack_detection_model8/weights/best.pt --img 640 --source /content/pipeline/pipeline/Test/Images --save-txt --save-crop --project /content/yolov5_testing

predicted_images_dir = '/content/yolov5_testing/'
actual_labels_dir = '/content/pipeline/pipeline/Test/Labels'
images_dir = '/content/yolov5_testing/exp/'
labels_dir = '/content/yolov5_testing/exp/labels'

# Load actual labels
def load_labels(label_path):
    with open(label_path, 'r') as file:
        labels = file.readlines()
    return [list(map(float, label.strip().split())) for label in labels]

# Extract bounding boxes from label files
def extract_bboxes(label_path):
    with open(label_path, 'r') as file:
        lines = file.readlines()
    bboxes = []
    for line in lines:
        parts = line.strip().split()

        class_id, x_center, y_center, width, height = map(float, parts)
        bboxes.append((class_id, x_center, y_center, width, height))
    return bboxes

# Plot bounding boxes on the image
def plot_bboxes(img, labels, color='red', title="Predicted Image"):
    fig, ax = plt.subplots(1, figsize=(10, 10))
    ax.imshow(img)
    ax.axis('off')
    ax.set_title(title)

    for label in labels:

        x_center, y_center, width, height = label[1], label[2], label[3], label[4]
        left = (x_center - width / 2) * img.width
        top = (y_center - height / 2) * img.height
        right = (x_center + width / 2) * img.width
        bottom = (y_center + height / 2) * img.height
        ax.add_patch(plt.Rectangle((left, top), right-left, bottom-top, linewidth=2, edgecolor=color, facecolor='none'))

    plt.show()

# Show the detected image with corresponding label
def show_detected_image(image_path, label_path, predicted_bboxes, actual_bboxes):
    img = Image.open(image_path)
    img = np.array(img)

    # Display the image
    plt.figure(figsize=(10, 10))
    plt.imshow(img)
    plt.axis('off')
    plt.title(f'Detected Image: {os.path.basename(image_path)}')

    # Plot actual bounding boxes
    for bbox in actual_bboxes:
        class_id, x_center, y_center, width, height = bbox
        x1 = (x_center - width / 2) * img.shape[1]
        y1 = (y_center - height / 2) * img.shape[0]
        x2 = (x_center + width / 2) * img.shape[1]
        y2 = (y_center + height / 2) * img.shape[0]
        plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='blue', facecolor='none', label='Actual'))

    # Plot predicted bounding boxes
    for bbox in predicted_bboxes:
        class_id, x_center, y_center, width, height = bbox
        x1 = (x_center - width / 2) * img.shape[1]
        y1 = (y_center - height / 2) * img.shape[0]
        x2 = (x_center + width / 2) * img.shape[1]
        y2 = (y_center + height / 2) * img.shape[0]
        plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='dashed', label='Predicted'))

    plt.legend(loc="upper left")
    plt.show()

# List of image IDs to visualize
image_ids = [451, 453, 454, 455]

# Loop through the image IDs and display results
for image_id in image_ids:

    image_path = os.path.join(predicted_images_dir, f"exp/{image_id:04d}.png")  # Predicted image
    actual_label_path = os.path.join(actual_labels_dir, f"{image_id:04d}.txt")  # Actual ground truth label
    predicted_label_path = os.path.join(predicted_images_dir, f"labels/{image_id:04d}.txt")  # Predicted label
    actual_labels = load_labels(actual_label_path) if os.path.exists(actual_label_path) else []
    predicted_labels = load_labels(predicted_label_path) if os.path.exists(predicted_label_path) else []
    img = Image.open(image_path)
    plot_bboxes(img, predicted_labels, color='green', title=f"Predicted Bounding Boxes: {image_id}")
    plot_bboxes(img, actual_labels, color='red', title=f"Actual Bounding Boxes: {image_id}")
